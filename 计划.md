### 第 1-2 周：打基础（PyTorch 核心）

🎯 目标：熟悉张量、自动求导、模型训练流程。
- 理论
- 张量（Tensor）基本操作
- Autograd 机制（requires_grad, backward）
- nn.Module & torch.optim
- Dataset & DataLoader
- 实践项目
	1.	手写一个线性回归（拟合 y = 3x+2）
	2.	手写逻辑回归做二分类（比如二维点分类）
- 产出
- 总结笔记：常用 tensor 操作表
- 掌握完整训练循环（forward → loss → backward → step）

⸻

### 第 3-4 周：入门项目（图像 & NLP）

🎯 目标：掌握常见网络，能处理公开数据集。
- 理论
- CNN（卷积、池化）
- RNN / LSTM（序列建模）
- torchvision / torchtext 使用方法
- 实践项目
	1.	MNIST 手写数字识别（CNN）
	2.	IMDB 情感分类（LSTM/GRU）
- 产出
- 模型训练 & 测试准确率
- 学会保存和加载模型（torch.save, torch.load）

⸻

### 第 5-6 周：进阶（预训练模型 & Transformer）

🎯 目标：会用预训练模型，接触现代 NLP 模型。
- 理论
- 迁移学习（finetune / feature extraction）
- Transformer 架构（Self-Attention）
- Hugging Face Transformers
- 实践项目
	1.	使用预训练 ResNet 微调 CIFAR10 分类
	2.	使用预训练 BERT 做文本分类（Hugging Face）
- 产出
- 能调用现成模型并迁移到新任务
- 学会查看模型结构（model.named_parameters()）

⸻

### 第 7 周：优化与大模型训练

🎯 目标：学习提升训练效率与规模化。
- 理论
- Mixed Precision（torch.cuda.amp）
- 分布式训练（DDP/FSDP）
- 模型压缩（量化/剪枝/蒸馏）
- 实践项目
	1.	在 GPU 上用混合精度训练 MNIST
	2.	模拟分布式训练（2 GPU，DDP）
- 产出
- 总结训练加速技巧
- 学会查看 GPU 占用（nvidia-smi）

⸻

### 第 8 周：部署与综合项目

🎯 目标：能把模型跑起来 & 服务化。
- 理论
- TorchScript (torch.jit.trace)
- ONNX 导出 & 推理
- TorchServe 部署
- 实践项目
	1.	导出训练好的模型到 ONNX，写一个 Flask/FastAPI Web 接口
	2.	用 TorchServe 部署 MNIST 分类服务
- 产出
- 可以让别人调用你训练好的模型（HTTP API）
- 写一篇总结博客《我的 8 周 PyTorch 学习之路》